{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import BertTokenizerFast\n",
    "import copy\n",
    "import torch\n",
    "from common.utils import Preprocessor\n",
    "import yaml\n",
    "import logging\n",
    "from pprint import pprint\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "config = yaml.load(open(\"build_data_config.yaml\", \"r\"), Loader = yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(filepath, by_line=True):\n",
    "    data = []\n",
    "    with open(filepath, 'r') as handle:\n",
    "        if by_line:\n",
    "            for line in handle:\n",
    "                data.append(\n",
    "                    json.loads(line)\n",
    "                )\n",
    "        else:\n",
    "            data = json.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "exp_name = config[\"exp_name\"]\n",
    "# data_in_dir = '/Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_star'\n",
    "# data_out_dir = '/Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_star'\n",
    "data_in_dir = os.path.join(config[\"data_in_dir\"])\n",
    "data_out_dir = os.path.join(config[\"data_out_dir\"])\n",
    "if not os.path.exists(data_out_dir):\n",
    "    os.makedirs(data_out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_raw'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_in_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_train.json\n",
      "/Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_raw/raw_train.json\n",
      "raw_test.json\n",
      "/Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_raw/raw_test.json\n",
      "raw_valid.json\n",
      "/Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_raw/raw_valid.json\n"
     ]
    }
   ],
   "source": [
    "file_name2data = {}\n",
    "for path, folds, files in os.walk(data_in_dir):\n",
    "    for file_name in files:\n",
    "        print(file_name)\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        file_name = re.match(\"(.*?)\\.json\", file_name).group(1)\n",
    "        print(file_path)\n",
    "        file_name2data[file_name] = load_json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 56196 | Dev: 5000 | Test: 5000\n"
     ]
    }
   ],
   "source": [
    "train_data = file_name2data['raw_train']\n",
    "dev_data = file_name2data['raw_valid']\n",
    "test_data = file_name2data['raw_test']\n",
    "print('Train: {} | Dev: {} | Test: {}'.format(\n",
    "    len(train_data), len(dev_data), len(test_data)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sentText', 'articleId', 'relationMentions', 'entityMentions', 'sentId'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0, 'label': 'LOCATION', 'text': 'Columbus'},\n",
       " {'start': 1, 'label': 'LOCATION', 'text': 'Ohio'},\n",
       " {'start': 2, 'label': 'PERSON', 'text': 'Zach Wells'},\n",
       " {'start': 3, 'label': 'PERSON', 'text': 'Kyle Martino'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[3]['entityMentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'em1Text': 'Anthony D. Weiner',\n",
       "  'em2Text': 'Brooklyn',\n",
       "  'label': '/people/person/place_lived'},\n",
       " {'em1Text': 'Anthony D. Weiner',\n",
       "  'em2Text': 'Queens',\n",
       "  'label': '/people/person/place_lived'}]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[6]['relationMentions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0, 'label': 'LOCATION', 'text': 'Debra Hill'},\n",
       " {'start': 1, 'label': 'LOCATION', 'text': 'Haddonfield'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[2]['entityMentions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# @specific\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "bert_path = 'bert-large-cased'\n",
    "if config[\"encoder\"] == \"BERT\":\n",
    "#     tokenizer = BertTokenizerFast.from_pretrained(config[\"bert_path\"], add_special_tokens = False, do_lower_case = False)\n",
    "    tokenizer = BertTokenizerFast.from_pretrained(bert_path, add_special_tokens = False, do_lower_case = False)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "    tokenize = tokenizer.tokenize\n",
    "    get_tok2char_span_map = lambda text: tokenizer.encode_plus(text, return_offsets_mapping = True, add_special_tokens = False)[\"offset_mapping\"]\n",
    "elif config[\"encoder\"] == \"BiLSTM\":\n",
    "    tokenize = lambda text: text.split(\" \")\n",
    "    def get_tok2char_span_map(text):\n",
    "        tokens = tokenize(text)\n",
    "        tok2char_span = []\n",
    "        char_num = 0\n",
    "        for tok in tokens:\n",
    "            tok2char_span.append((char_num, char_num + len(tok)))\n",
    "            char_num += len(tok) + 1 # +1: whitespace\n",
    "        return tok2char_span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessor(tokenize_func = tokenize, \n",
    "                            get_tok2char_span_map_func = get_tok2char_span_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Transforming data format: 56196it [00:00, 445318.56it/s]\n",
      "Clean:   0%|          | 0/56196 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_nyt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clean: 100%|██████████| 56196/56196 [00:00<00:00, 236967.80it/s]\n",
      "Transforming data format: 5000it [00:00, 557412.22it/s]\n",
      "Clean: 100%|██████████| 5000/5000 [00:00<00:00, 193805.69it/s]\n",
      "Transforming data format: 5000it [00:00, 358420.13it/s]\n",
      "Clean: 100%|██████████| 5000/5000 [00:00<00:00, 185337.73it/s]\n"
     ]
    }
   ],
   "source": [
    "ori_format = config[\"ori_data_format\"]\n",
    "ori_format = 'raw_nyt'\n",
    "print(ori_format)\n",
    "if ori_format != \"tplinker\": # if tplinker, skip transforming\n",
    "    for file_name, data in file_name2data.items():\n",
    "        if \"train\" in file_name:\n",
    "            data_type = \"train\"\n",
    "        if \"valid\" in file_name:\n",
    "            data_type = \"valid\"\n",
    "        if \"test\" in file_name:\n",
    "            data_type = \"test\"\n",
    "        data = preprocessor.transform_data(data, ori_format = ori_format, dataset_type = data_type, add_id = True)\n",
    "        file_name2data[file_name] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 56196 | Dev: 5000 | Test: 5000\n"
     ]
    }
   ],
   "source": [
    "train_data = file_name2data['raw_train']\n",
    "dev_data = file_name2data['raw_valid']\n",
    "test_data = file_name2data['raw_test']\n",
    "print('Train: {} | Dev: {} | Test: {}'.format(\n",
    "    len(train_data), len(dev_data), len(test_data)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'North Carolina EASTERN MUSIC FESTIVAL Greensboro , June 25-July 30 .',\n",
       " 'id': 'train_1',\n",
       " 'relation_list': [{'subject': 'North Carolina',\n",
       "   'predicate': '/location/location/contains',\n",
       "   'object': 'Greensboro'}]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Clean and Add Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# check token level span\n",
    "def check_tok_span(data):\n",
    "    def extr_ent(text, tok_span, tok2char_span):\n",
    "        char_span_list = tok2char_span[tok_span[0]:tok_span[1]]\n",
    "        char_span = (char_span_list[0][0], char_span_list[-1][1])\n",
    "        decoded_ent = text[char_span[0]:char_span[1]]\n",
    "        return decoded_ent\n",
    "\n",
    "    span_error_memory = set()\n",
    "    for sample in tqdm(data, desc = \"check tok spans\"):\n",
    "        text = sample[\"text\"]\n",
    "        tok2char_span = get_tok2char_span_map(text)\n",
    "        for rel in sample[\"relation_list\"]:\n",
    "            subj_tok_span, obj_tok_span = rel[\"subj_tok_span\"], rel[\"obj_tok_span\"]\n",
    "            if extr_ent(text, subj_tok_span, tok2char_span) != rel[\"subject\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, subj_tok_span, tok2char_span), rel[\"subject\"]))\n",
    "            if extr_ent(text, obj_tok_span, tok2char_span) != rel[\"object\"]:\n",
    "                span_error_memory.add(\"extr: {}---gold: {}\".format(extr_ent(text, obj_tok_span, tok2char_span), rel[\"object\"]))\n",
    "                \n",
    "    return span_error_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clean data: 100%|██████████| 56196/56196 [00:00<00:00, 61059.06it/s]\n",
      "clean data w char spans: 100%|██████████| 56196/56196 [00:00<00:00, 94366.13it/s]\n",
      "Adding token level spans: 100%|██████████| 56032/56032 [00:03<00:00, 15991.43it/s]\n",
      "collect relations: 100%|██████████| 56032/56032 [00:00<00:00, 1179345.44it/s]\n",
      "check tok spans: 100%|██████████| 56032/56032 [00:00<00:00, 81667.28it/s]\n",
      "clean data: 100%|██████████| 5000/5000 [00:00<00:00, 56174.07it/s]\n",
      "clean data w char spans: 100%|██████████| 5000/5000 [00:00<00:00, 156289.27it/s]\n",
      "Adding token level spans:   0%|          | 0/4979 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extr: Brooklynites---gold: Brooklyn', 'extr: Austrians---gold: Austria', 'extr: Oregonian---gold: Oregon', 'extr: Sukarnoputri---gold: Sukarno', 'extr: Congressional---gold: Congress', 'extr: Kyrgyzstan---gold: Kyrgyz', 'extr: Germanys---gold: Germany', 'extr: Chinatown---gold: China', 'extr: Africans---gold: Africa', 'extr: Australian---gold: Australia', 'extr: Cambodian---gold: Cambodia', 'extr: Japanese---gold: Japan', 'extr: Rwandan---gold: Rwanda', 'extr: Brooklyn-Queens---gold: Brooklyn', 'extr: Russian---gold: Russia', 'extr: Brooklyn-Queens---gold: Queens', 'extr: Iraqi-manned---gold: Iraq', 'extr: Miami-Dade---gold: Miami', 'extr: Vietnamese---gold: Vietnam', 'extr: Taiwanese---gold: Taiwan', 'extr: Rutgers-Newark---gold: Newark', 'extr: Israelis---gold: Israel', 'extr: Arab-Israeli---gold: Israel', 'extr: Zimbabwean---gold: Zimbabwe', 'extr: Chicago-based---gold: Chicago', 'extr: Catalonian---gold: Catalonia', 'extr: Indianapolis---gold: Indiana', 'extr: Brooklyn-born---gold: Brooklyn', 'extr: Israeli-Palestinian---gold: Israel', 'extr: Harare-area---gold: Harare', 'extr: Kansas-as-the-center-of-the-universe---gold: Kansas', 'extr: North Korean---gold: North Korea', 'extr: Russian-born---gold: Russia', 'extr: Newton-Brookline---gold: Newton', 'extr: Manhattans---gold: Manhattan', 'extr: Jackson-George---gold: Jackson', 'extr: Maine-grown---gold: Maine', 'extr: Liberians---gold: Liberia', 'extr: Parmalat---gold: Parma', 'extr: Chilean-American---gold: Chile', 'extr: Icelandair---gold: Iceland', 'extr: South African---gold: South Africa', 'extr: Acehnese---gold: Aceh', 'extr: Rhode Islanders---gold: Rhode Island', 'extr: Austrian---gold: Austria', 'extr: Islamists---gold: Islam', 'extr: al-Zarqawi---gold: Zarqa', 'extr: Manhattanville---gold: Manhattan', 'extr: New Yorkers---gold: New York', 'extr: Liberian---gold: Liberia', 'extr: Iran-Iraq---gold: Iran', 'extr: Ex-Ukraine---gold: Ukraine', 'extr: Mainers---gold: Maine', 'extr: Mississippian---gold: Mississippi', 'extr: Latin Americans---gold: Latin America', 'extr: Muslims---gold: Muslim', 'extr: Russians---gold: Russia', 'extr: Kenyatta---gold: Kenya', 'extr: Jordanian---gold: Jordan', 'extr: Stratford-on-Avon---gold: Stratford', 'extr: Egyptian---gold: Egypt', 'extr: African-American---gold: Africa', 'extr: New Yorker---gold: New York', 'extr: Newarks---gold: Newark', 'extr: Cuban---gold: Cuba', 'extr: Bolivian---gold: Bolivia', 'extr: South Koreans---gold: South Korea', 'extr: European---gold: Europe', 'extr: non-Florida---gold: Florida', 'extr: Iranian-Canadian---gold: Iran', 'extr: Southeast Asian---gold: Southeast Asia', 'extr: Iraqis---gold: Iraq', 'extr: Galician---gold: Galicia', 'extr: Europe-wide---gold: Europe', 'extr: Russias---gold: Russia', 'extr: African---gold: Africa', 'extr: Pakistani-administered---gold: Pakistan', 'extr: Bangladeshi---gold: Bangladesh', 'extr: British-Iraqi---gold: Iraq', 'extr: Palestinian-Israeli---gold: Israel', 'extr: Italy-related---gold: Italy', 'extr: Asians---gold: Asia', 'extr: Atlanta-Tel Aviv---gold: Tel Aviv', 'extr: Syrians---gold: Syria', 'extr: Queensborough---gold: Queens', 'extr: Egyptian-born---gold: Egypt', 'extr: Oklahoman---gold: Oklahoma', 'extr: Perm-36---gold: Perm', 'extr: Craigslist.org---gold: Craigslist', 'extr: Croatian---gold: Croatia', 'extr: Towsontown---gold: Towson', 'extr: Algerians---gold: Algeria', 'extr: India-Pakistan---gold: India', 'extr: Serbian---gold: Serbia', 'extr: Hill\\\\/Brooklyn---gold: Brooklyn', 'extr: Iraqi---gold: Iraq', 'extr: Nepali---gold: Nepal', 'extr: Bulgarian---gold: Bulgaria', 'extr: Kenyan---gold: Kenya', 'extr: Bahian---gold: Bahia', 'extr: China-based---gold: China', 'extr: Chilean---gold: Chile', 'extr: Europeans---gold: Europe', 'extr: Colombian---gold: Colombia', 'extr: Bayreuther---gold: Bayreuth', 'extr: Hersheypark---gold: Hershey', 'extr: Northport---gold: North', 'extr: Congressman---gold: Congress', 'extr: U.S.-China---gold: China', 'extr: Guineau-Bissau---gold: Guinea', 'extr: Victorian---gold: Victoria', 'extr: Pakistani---gold: Pakistan', 'extr: Nicaraguan---gold: Nicaragua', 'extr: Zarqawi---gold: Zarqa', 'extr: Mid-Ohio---gold: Ohio', 'extr: Icelandic---gold: Iceland', 'extr: Iranians---gold: Iran', 'extr: Francesco---gold: France', 'extr: Cubans---gold: Cuba', 'extr: Microsoft-Google---gold: Microsoft', 'extr: Pakistanis---gold: Pakistan', 'extr: Iraqi-Canadian---gold: Iraq', 'extr: Hawaiian---gold: Hawaii', 'extr: Bangkok-Singapore---gold: Singapore', 'extr: Technion-Israel---gold: Israel', 'extr: Canada-France-Hawaii---gold: Hawaii', 'extr: Ugandan---gold: Uganda', 'extr: Nigerian---gold: Nigeria', 'extr: States-India---gold: India', 'extr: Iranian-born---gold: Iran', 'extr: Romanian---gold: Romania', 'extr: non-European---gold: Europe', 'extr: Iranian---gold: Iran', 'extr: Sri Lankan---gold: Sri Lanka', 'extr: Union-United Nations---gold: United Nations', 'extr: Zopa.com---gold: Zopa', 'extr: Nagoya\\\\/Boston---gold: Nagoya', 'extr: Venezuelan---gold: Venezuela', 'extr: Angolan---gold: Angola', 'extr: Baghrani---gold: Baghran', 'extr: Israeli-Arab---gold: Israel', 'extr: Indians---gold: India', 'extr: Jurancon---gold: Jura', 'extr: Israel-Palestine---gold: Israel', 'extr: Indian---gold: India', 'extr: Ohioans---gold: Ohio', 'extr: Kenyan-born---gold: Kenya', 'extr: Indonesians---gold: Indonesia', 'extr: Long Island-grown---gold: Long Island', 'extr: Microsoft-Google---gold: Google', 'extr: Israeli---gold: Israel', 'extr: Pennsylvania---gold: Penn', 'extr: Bigelow-Hartford---gold: Hartford', 'extr: Peruvian---gold: Peru', 'extr: Scripps\\\\/Shopzilla---gold: Shopzilla', 'extr: Bolivians---gold: Bolivia', 'extr: Indonesian---gold: Indonesia', 'extr: Waldorf-Astoria---gold: Astoria', 'extr: Iran-Iraq---gold: Iraq', 'extr: GayWhistler---gold: Whistler', 'extr: Asian---gold: Asia', 'extr: Brazilian---gold: Brazil', 'extr: Libyan---gold: Libya', 'extr: Sudanese---gold: Sudan', 'extr: Manhattan-based---gold: Manhattan', 'extr: Syrian---gold: Syria', 'extr: China-made---gold: China', 'extr: All-Russia---gold: Russia'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding token level spans: 100%|██████████| 4979/4979 [00:00<00:00, 16462.72it/s]\n",
      "collect relations: 100%|██████████| 4979/4979 [00:00<00:00, 877824.28it/s]\n",
      "check tok spans: 100%|██████████| 4979/4979 [00:00<00:00, 70464.32it/s]\n",
      "clean data: 100%|██████████| 5000/5000 [00:00<00:00, 53364.00it/s]\n",
      "clean data w char spans: 100%|██████████| 5000/5000 [00:00<00:00, 167854.07it/s]\n",
      "Adding token level spans:   0%|          | 0/4983 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extr: New Yorkers---gold: New York', 'extr: Israeli-Lebanon---gold: Lebanon', 'extr: Ex-Enron---gold: Enron', 'extr: Asia-Pacific---gold: Asia', 'extr: Queensborough---gold: Queens', 'extr: one-China---gold: China', 'extr: Texas-Oklahoma---gold: Texas', 'extr: Africans---gold: Africa', 'extr: Australian---gold: Australia', 'extr: Japanese---gold: Japan', 'extr: Iranian---gold: Iran', 'extr: Russian---gold: Russia', 'extr: Miami-Dade---gold: Miami', 'extr: Serbian---gold: Serbia', 'extr: Iraqi---gold: Iraq', 'extr: Taiwanese---gold: Taiwan', 'extr: New Jersey-oriented---gold: New Jersey', 'extr: Cambodian-born---gold: Cambodia', 'extr: New Zealander---gold: New Zealand', 'extr: Israelis---gold: Israel', 'extr: Arab-Israeli---gold: Israel', 'extr: Tasmanian---gold: Tasmania', 'extr: Indians---gold: India', 'extr: Zambian---gold: Zambia', 'extr: Chilean---gold: Chile', 'extr: Newarks---gold: Newark', 'extr: Indian---gold: India', 'extr: Cuban---gold: Cuba', 'extr: European---gold: Europe', 'extr: Indianapolis---gold: Indiana', 'extr: Iraqis---gold: Iraq', 'extr: Israeli---gold: Israel', 'extr: Russia-Japan---gold: Russia', 'extr: Indonesian---gold: Indonesia', 'extr: African---gold: Africa', 'extr: Pakistani---gold: Pakistan', 'extr: Olympian---gold: Olympia', 'extr: Russia-Georgia---gold: Russia', 'extr: Asian---gold: Asia', 'extr: Hollywoodland---gold: Hollywood', 'extr: Brazilian---gold: Brazil', 'extr: Syrian---gold: Syria', 'extr: Cuban-American---gold: Cuba'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding token level spans: 100%|██████████| 4983/4983 [00:00<00:00, 16941.90it/s]\n",
      "collect relations: 100%|██████████| 4983/4983 [00:00<00:00, 1108118.17it/s]\n",
      "check tok spans: 100%|██████████| 4983/4983 [00:00<00:00, 81316.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extr: New Yorkers---gold: New York', 'extr: Queensborough---gold: Queens', 'extr: Austrians---gold: Austria', 'extr: Liberian---gold: Liberia', 'extr: anti-Syrian---gold: Syria', 'extr: Chinatown---gold: China', 'extr: non-European---gold: Europe', 'extr: Australian---gold: Australia', 'extr: Japanese---gold: Japan', 'extr: Iranian---gold: Iran', 'extr: Russian---gold: Russia', 'extr: Iraqi---gold: Iraq', 'extr: Urumqi-Beijing---gold: Beijing', 'extr: Miller-Great Neck---gold: Great Neck', 'extr: Israelis---gold: Israel', 'extr: Indian-restaurant-packed---gold: India', 'extr: New Yorker---gold: New York', 'extr: Indian---gold: India', 'extr: Cuban---gold: Cuba', 'extr: Bolivian---gold: Bolivia', 'extr: European---gold: Europe', 'extr: Russian-Israeli---gold: Israel', 'extr: South Korean---gold: South Korea', 'extr: Israeli-Palestinian---gold: Israel', 'extr: Iraqis---gold: Iraq', 'extr: Israeli---gold: Israel', 'extr: African---gold: Africa', 'extr: Pakistani---gold: Pakistan', 'extr: Islamist---gold: Islam', 'extr: South African---gold: South Africa', 'extr: Austrian---gold: Austria', 'extr: Brazilian---gold: Brazil', 'extr: Hawaiian---gold: Hawaii', 'extr: Sudanese---gold: Sudan', 'extr: Libyans---gold: Libya', 'extr: Syrian---gold: Syria', 'extr: Cuban-American---gold: Cuba', 'extr: China-Japan---gold: China'}\n",
      "{'raw_test': {'char_span_error': 25, 'tok_span_error': 43},\n",
      " 'raw_train': {'char_span_error': 194, 'tok_span_error': 168},\n",
      " 'raw_valid': {'char_span_error': 23, 'tok_span_error': 38}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# clean, add char span, tok span\n",
    "# collect relations\n",
    "# check tok spans\n",
    "rel_set = set()\n",
    "error_statistics = {}\n",
    "for file_name, data in file_name2data.items():\n",
    "    assert len(data) > 0\n",
    "    if \"relation_list\" in data[0]: # train or valid data\n",
    "        # rm redundant whitespaces\n",
    "        # separate by whitespaces\n",
    "        data = preprocessor.clean_data_wo_span(data, separate = config[\"separate_char_by_white\"])\n",
    "        error_statistics[file_name] = {}\n",
    "        # add char span\n",
    "        if config[\"add_char_span\"]:\n",
    "            data, miss_sample_list = preprocessor.add_char_span(data, config[\"ignore_subword\"])\n",
    "            error_statistics[file_name][\"miss_samples\"] = len(miss_sample_list)\n",
    "            \n",
    "        # clean\n",
    "        data, bad_samples_w_char_span_error = preprocessor.clean_data_w_span(data)\n",
    "        error_statistics[file_name][\"char_span_error\"] = len(bad_samples_w_char_span_error)\n",
    "        \n",
    "        # add tok span\n",
    "        data = preprocessor.add_tok_span(data)\n",
    "        \n",
    "        # collect relations\n",
    "        for sample in tqdm(data, desc = \"collect relations\"):\n",
    "            for rel in sample[\"relation_list\"]:\n",
    "                rel_set.add(rel[\"predicate\"])\n",
    "        \n",
    "        # check tok span\n",
    "        if config[\"check_tok_span\"]:\n",
    "            span_error_memory = check_tok_span(data)\n",
    "            if len(span_error_memory) > 0:\n",
    "                print(span_error_memory)\n",
    "            error_statistics[file_name][\"tok_span_error\"] = len(span_error_memory)\n",
    "            \n",
    "        file_name2data[file_name] = data\n",
    "pprint(error_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 56032 | Dev: 4983 | Test: 4979\n"
     ]
    }
   ],
   "source": [
    "train_data = file_name2data['raw_train']\n",
    "dev_data = file_name2data['raw_valid']\n",
    "test_data = file_name2data['raw_test']\n",
    "print('Train: {} | Dev: {} | Test: {}'.format(\n",
    "    len(train_data), len(dev_data), len(test_data)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['raw_train', 'raw_test', 'raw_valid'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name2data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Massachusetts ASTON MAGNA Great Barrington ; also at Bard College , Annandale-on-Hudson , N.Y. , July 1-Aug .',\n",
       " 'id': 'train_0',\n",
       " 'relation_list': [{'subject': 'Annandale-on-Hudson',\n",
       "   'predicate': '/location/location/contains',\n",
       "   'object': 'Bard College',\n",
       "   'subj_char_span': [68, 87],\n",
       "   'obj_char_span': [53, 65],\n",
       "   'subj_tok_span': [11, 12],\n",
       "   'obj_tok_span': [8, 10]}]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name2data['raw_train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:raw_train is output to /Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_bilstm/raw_train.json\n",
      "INFO:root:raw_test is output to /Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_bilstm/raw_test.json\n",
      "INFO:root:raw_valid is output to /Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_bilstm/raw_valid.json\n",
      "INFO:root:rel2id is output to /Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_bilstm/rel2id.json\n",
      "INFO:root:data_statistics is output to /Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_bilstm/data_statistics.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'raw_test': 4979, 'raw_train': 56032, 'raw_valid': 4983, 'relation_num': 24}\n"
     ]
    }
   ],
   "source": [
    "rel_set = sorted(rel_set)\n",
    "rel2id = {rel:ind for ind, rel in enumerate(rel_set)}\n",
    "data_statistics = {\n",
    "    \"relation_num\": len(rel2id),\n",
    "}\n",
    "\n",
    "for file_name, data in file_name2data.items():\n",
    "    data_path = os.path.join(data_out_dir, \"{}.json\".format(file_name))\n",
    "    json.dump(data, open(data_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "    logging.info(\"{} is output to {}\".format(file_name, data_path))\n",
    "    data_statistics[file_name] = len(data)\n",
    "\n",
    "rel2id_path = os.path.join(data_out_dir, \"rel2id.json\")\n",
    "json.dump(rel2id, open(rel2id_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False)\n",
    "logging.info(\"rel2id is output to {}\".format(rel2id_path))\n",
    "\n",
    "data_statistics_path = os.path.join(data_out_dir, \"data_statistics.txt\")\n",
    "json.dump(data_statistics, open(data_statistics_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "logging.info(\"data_statistics is output to {}\".format(data_statistics_path)) \n",
    "\n",
    "pprint(data_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genrate WordDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 100%|██████████| 65994/65994 [00:00<00:00, 90326.63it/s]\n",
      "Filter uncommon words: 100%|██████████| 90818/90818 [00:00<00:00, 1714357.79it/s]\n",
      "INFO:root:token2idx is output to /Users/georgestoica/Desktop/Research/TPlinker-joint-extraction/ori_data/nyt_bilstm/token2idx.json, total token num: 34790\n"
     ]
    }
   ],
   "source": [
    "if config[\"encoder\"] in {\"BiLSTM\", }:\n",
    "    all_data = []\n",
    "    for data in list(file_name2data.values()):\n",
    "        all_data.extend(data)\n",
    "        \n",
    "    token2num = {}\n",
    "    for sample in tqdm(all_data, desc = \"Tokenizing\"):\n",
    "        text = sample['text']\n",
    "        for tok in tokenize(text):\n",
    "            token2num[tok] = token2num.get(tok, 0) + 1\n",
    "    \n",
    "    token2num = dict(sorted(token2num.items(), key = lambda x: x[1], reverse = True))\n",
    "    max_token_num = 50000\n",
    "    token_set = set()\n",
    "    for tok, num in tqdm(token2num.items(), desc = \"Filter uncommon words\"):\n",
    "        if num < 3: # filter words with a frequency of less than 3\n",
    "            continue\n",
    "        token_set.add(tok)\n",
    "        if len(token_set) == max_token_num:\n",
    "            break\n",
    "        \n",
    "    token2idx = {tok:idx + 2 for idx, tok in enumerate(sorted(token_set))}\n",
    "    token2idx[\"<PAD>\"] = 0\n",
    "    token2idx[\"<UNK>\"] = 1\n",
    "#     idx2token = {idx:tok for tok, idx in token2idx.items()}\n",
    "    \n",
    "    dict_path = os.path.join(data_out_dir, \"token2idx.json\")\n",
    "    json.dump(token2idx, open(dict_path, \"w\", encoding = \"utf-8\"), ensure_ascii = False, indent = 4)\n",
    "    logging.info(\"token2idx is output to {}, total token num: {}\".format(dict_path, len(token2idx))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}